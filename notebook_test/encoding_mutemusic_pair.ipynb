{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dba5b5ab-da1a-4930-be21-6cee2cfb0cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas, pickle, os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#ridge regression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "from torch import Tensor\n",
    "from torchvision.models import feature_extraction\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import encoding_utils as eu\n",
    "import models_class as mc\n",
    "import visualisation_utils as visu\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b789b6f-54c2-4ff3-a9a1-14edd9ea90d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------env args------------------------------------------------------------\n",
    "dataset = 'mutemusic'\n",
    "sub = 'sub-03'\n",
    "no_init = False\n",
    "tr=1.49\n",
    "\n",
    "#absolute paths\n",
    "model_path = '/home/maellef/Results/best_models/converted' \n",
    "training_data_path = '/home/maellef/git/MuteMusic_analysis/data/training_data'\n",
    "\n",
    "#specific to soundnet/audio\n",
    "model_type = 'soundnet' #'conv4'\n",
    "resolution = 'MIST_ROI'# 'auditory_Voxels' \n",
    "sr=22050\n",
    "\n",
    "#specific to one instance\n",
    "stim_tracks = 'silenced'\n",
    "category = 'all'\n",
    "repetition = 'all'\n",
    "original_sr = 48000\n",
    "\n",
    "#visualisation\n",
    "r2_max_threshold = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7a430d-a094-48f7-aecb-0cc40c340a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specific to mutemusic---------separate silence bold from music bold\n",
    "def pair_silence_with_music(track_data, metadata_df, sr=22050, bold=False):\n",
    "    paired_segments = []\n",
    "    for track, (i, metadata) in zip(track_data, metadata_df.iterrows()):\n",
    "        timestamps_s = {'duration':[metadata['S1_duration'], metadata['S2_duration'], metadata['S3_duration'], metadata['S4_duration']],\n",
    "                        'start':[metadata['S1_start'], metadata['S2_start'], metadata['S3_start'], metadata['S4_start']],\n",
    "                        'stop':[metadata['S1_stop'], metadata['S2_stop'], metadata['S3_stop'], metadata['S4_stop']]}\n",
    "        siltt_df = pandas.DataFrame(timestamps_s).sort_values(by='start').dropna()\n",
    "        music_start = 0\n",
    "        \n",
    "        for i, silence_tt in siltt_df.iterrows():            \n",
    "            start = silence_tt['start']/tr if bold else silence_tt['start']*sr\n",
    "            silence_start = round(start)\n",
    "            music_stop = silence_start\n",
    "\n",
    "            stop = silence_tt['stop']/tr if bold else silence_tt['stop']*sr\n",
    "            silence_stop = round(stop)\n",
    "\n",
    "            paired_segments.append((track[music_start:music_stop],\n",
    "                                   track[silence_start:silence_stop]))\n",
    "            music_start = silence_stop\n",
    "        \n",
    "    return paired_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e0f95d-ca0f-4900-addc-07ac0cc82ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------load training data-------------------------------------------------------\n",
    "#load data + metadata\n",
    "metadata_path = os.path.join(training_data_path, f'{dataset}_{sub}_{stim_tracks}_metadata.tsv')\n",
    "pairbold_path = os.path.join(training_data_path, f'{dataset}_{sub}_{stim_tracks}_pairWavBold')\n",
    "\n",
    "data_df = pandas.read_csv(metadata_path, sep='\\t')\n",
    "with open(pairbold_path, 'rb') as f: \n",
    "    wavbold = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638cf511-d84b-45b2-a272-fcc671d9358e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------load model + convert to extract embedding-------------------------------------\n",
    "#load model (specific to soundnet model)\n",
    "print(sub, resolution, model_type, category)\n",
    "model_name, model = eu.load_sub_model(sub, resolution, model_type, model_path, no_init=False)\n",
    "print(model_name)\n",
    "i = model_name.find('conv_') + len('conv_')\n",
    "temporal_size = int(model_name[i:i+3])\n",
    "\n",
    "#create model with extractable embeddings\n",
    "return_nodes = {'soundnet.conv7.2':'conv7', 'encoding_fmri':'encoding_conv'}\n",
    "model_feat = feature_extraction.create_feature_extractor(model, return_nodes=return_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8107e1e1-308b-44fe-b5e4-d654c63feac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create train/test for ridge regression\n",
    "\n",
    "train_data, train_data_df = eu.extract_selected_data(wavbold, data_df, repetition=[1,2])\n",
    "test_data, test_data_df = eu.extract_selected_data(wavbold, data_df, repetition=[3])\n",
    "f_test_data, f_test_data_df = eu.extract_selected_data(wavbold, data_df, repetition=[3], groupe='F')\n",
    "u_test_data, u_test_data_df = eu.extract_selected_data(wavbold, data_df, repetition=[3], groupe='U')\n",
    "\n",
    "datasets_dict = {\n",
    "    'train':(train_data, train_data_df),\n",
    "    'test':(test_data, test_data_df),\n",
    "    'test_F':(f_test_data, f_test_data_df),\n",
    "    'test_U':(u_test_data, u_test_data_df)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb5b53e-3237-4e98-ac0d-0b4cf1f0599c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataset\n",
    "embedding_dict = {}\n",
    "for dataset_type, (data, data_df) in datasets_dict.items():\n",
    "    wav_data = [x for (x,y) in data]\n",
    "    wav_mussil = pair_silence_with_music(wav_data, data_df, \n",
    "                                                   sr=original_sr, bold=False)    \n",
    "    bold_data = [y for (x,y) in data]\n",
    "    bold_mussil = pair_silence_with_music(bold_data, data_df,\n",
    "                                                     sr=original_sr, bold=True)\n",
    "    sil_tr_len = [bold_s.shape[0] for bold_m, bold_s in bold_mussil]\n",
    "\n",
    "    wav_paired = [np.concatenate([wav_m, wav_s]) for wav_m, wav_s in wav_mussil]\n",
    "    bold_paired = [np.concatenate([bold_m, bold_s]) for bold_m, bold_s in bold_mussil]\n",
    "\n",
    "    wavbold_data = [(wav, bold) for wav, bold in zip(wav_paired, bold_paired)]\n",
    "    #create embedding through pretrained network\n",
    "    encoding_dataset = mc.soundnet_dataset(wavbold_data, tr=tr, sr=sr)\n",
    "    if original_sr != encoding_dataset.sr:\n",
    "        encoding_dataset.resample_input(input_sr=original_sr)\n",
    "    encoding_dataset.convert_input_to_tensor()\n",
    "    \n",
    "    testloader = DataLoader(encoding_dataset)\n",
    "    out_p = eu.test(testloader, net=model_feat, return_nodes=return_nodes, gpu=False)\n",
    "    \n",
    "    Y_pred_converted, Y_real_converted = [], []\n",
    "    for y_p, y_r in out_p['conv7']:\n",
    "        (y_p_converted, y_r_converted) = encoding_dataset.redimension_output(y_p, y_r, cut='end')\n",
    "        Y_pred_converted.append(y_p_converted)\n",
    "        Y_real_converted.append(y_r_converted)\n",
    "    print(len(Y_pred_converted), Y_pred_converted[0].shape,\n",
    "         len(Y_real_converted), Y_real_converted[0].shape)\n",
    "\n",
    "    embedding_dict[dataset_type] = (Y_pred_converted, Y_real_converted, sil_tr_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a2676e-d629-4ed0-8bad-31d7dc0c7152",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d3b999-ca83-4e9e-9de9-f0499ac88b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding, bold, _ = embedding_dict['train']\n",
    "embedding = np.vstack(embedding)\n",
    "bold = np.vstack(bold)\n",
    "\n",
    "print(embedding.shape, bold.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c24ea19-3b8b-47d8-af40-2237424bfdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.logspace(0.1, 3, 10)\n",
    "model = RidgeCV(\n",
    "        alphas=alphas,\n",
    "        fit_intercept=True,\n",
    "        cv=10)\n",
    "\n",
    "model.fit(embedding, bold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de85384-0f91-43dd-841b-a76553dc6af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x, test_y, len_sil_tr = embedding_dict['test']\n",
    "print(len(test_x), len(test_y), len(len_sil_tr))\n",
    "\n",
    "test_fullx = np.vstack(test_x)\n",
    "test_fully = np.vstack(test_y)\n",
    "\n",
    "test_musx = np.vstack([x[:-len] for x, len in zip(test_x, len_sil_tr)])\n",
    "test_silx = np.vstack([x[-len:] for x, len in zip(test_x, len_sil_tr)])\n",
    "\n",
    "test_musy = np.vstack([y[:-len] for y, len in zip(test_y, len_sil_tr)])\n",
    "test_sily = np.vstack([y[-len:] for y, len in zip(test_y, len_sil_tr)])\n",
    "\n",
    "print(test_fullx.shape, test_fully.shape,\n",
    "     test_musx.shape, test_musy.shape,\n",
    "     test_silx.shape, test_sily.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ada619-b63f-4aa4-83e2-e0d32b1c459a",
   "metadata": {},
   "outputs": [],
   "source": [
    "testf_x, testf_y, lenf_sil_tr = embedding_dict['test_F']\n",
    "\n",
    "testf_fullx = np.vstack(testf_x)\n",
    "testf_fully = np.vstack(testf_y)\n",
    "\n",
    "testf_musx = np.vstack([x[:-len] for x, len in zip(testf_x, lenf_sil_tr)])\n",
    "testf_silx = np.vstack([x[-len:] for x, len in zip(testf_x, lenf_sil_tr)])\n",
    "\n",
    "testf_musy = np.vstack([y[:-len] for y, len in zip(testf_y, lenf_sil_tr)])\n",
    "testf_sily = np.vstack([y[-len:] for y, len in zip(testf_y, lenf_sil_tr)])\n",
    "\n",
    "print(testf_fullx.shape, testf_fully.shape,\n",
    "     testf_musx.shape, testf_musy.shape,\n",
    "     testf_silx.shape, testf_sily.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acc78aa-590c-4c17-ab02-a582aad3e940",
   "metadata": {},
   "outputs": [],
   "source": [
    "testu_x, testu_y, lenu_sil_tr = embedding_dict['test_U']\n",
    "\n",
    "testu_fullx = np.vstack(testu_x)\n",
    "testu_fully = np.vstack(testu_y)\n",
    "\n",
    "testu_musx = np.vstack([x[:-len] for x, len in zip(testu_x, lenu_sil_tr)])\n",
    "testu_silx = np.vstack([x[-len:] for x, len in zip(testu_x, lenu_sil_tr)])\n",
    "\n",
    "testu_musy = np.vstack([y[:-len] for y, len in zip(testu_y, lenu_sil_tr)])\n",
    "testu_sily = np.vstack([y[-len:] for y, len in zip(testu_y, lenu_sil_tr)])\n",
    "\n",
    "print(testu_fullx.shape, testu_fully.shape,\n",
    "     testu_musx.shape, testu_musy.shape,\n",
    "     testu_silx.shape, testu_sily.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8311a191-adc6-4715-a50c-37d1251e5617",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p = model.predict(test_fullx)\n",
    "r2 = r2_score(test_fully, y_p, multioutput='raw_values')\n",
    "r2 = np.where(r2<0, 0, r2)\n",
    "print(max(r2))\n",
    "colormap = visu.extend_colormap(original_colormap='turbo',\n",
    "                          percent_start = 0.1, percent_finish=0)\n",
    "visu.surface_fig(r2, vmax=0.4, threshold=0.005, cmap='turbo', symmetric_cbar=False)\n",
    "\n",
    "savepath = f'./figures/{sub}_{dataset}_{model_type}_paired_mussil_predict_mussil_HRF2.png'\n",
    "plt.savefig(savepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
