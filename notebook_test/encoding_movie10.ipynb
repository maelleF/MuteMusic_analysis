{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5947b706-56b0-47c6-9a6d-1f211e32fa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch, torchvision, pandas, pickle\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "from torchvision.models import feature_extraction\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#ridge regression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/maellef/git/cNeuromod_encoding_2020')\n",
    "from models import encoding_models as encod\n",
    "\n",
    "#visualisation\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors, colormaps\n",
    "#brain visualization import\n",
    "from nilearn import regions, datasets, surface, plotting, image, maskers\n",
    "from nilearn.plotting import plot_roi, plot_stat_map\n",
    "MIST_path = '/home/maellef/DataBase/fMRI_parcellations/MIST_parcellation/Parcellations/MIST_ROI.nii.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeba8346-ef84-4c51-a699-38e240dda2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def surface_fig(parcel_data, vmax, threshold=0, cmap='turbo', inflate=True, colorbar=True, \n",
    "                no_background=True, symmetric_cbar=True):\n",
    "    nii_data = regions.signals_to_img_labels(parcel_data, MIST_path)\n",
    "    fig, ax = plotting.plot_img_on_surf(nii_data,\n",
    "                              views=['lateral', 'medial'], hemispheres=['left', 'right'], inflate=inflate,\n",
    "                              vmax=vmax, threshold=threshold, colorbar=colorbar, cmap=cmap, \n",
    "                                        symmetric_cbar=symmetric_cbar, cbar_tick_format=\"%.1f\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66699a39-6d9a-4967-9f1a-53a73f9b714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_colormap(original_colormap = 'twilight', percent_start = 0.25, percent_finish = 0.25):\n",
    "    colormap = colormaps[original_colormap]\n",
    "    nb_colors = colormap.N\n",
    "    new_colors_range = colormap(np.linspace(0,1,nb_colors))\n",
    "\n",
    "    n_start = round(nb_colors/(1-percent_start)) - nb_colors if percent_start != 0 else 0\n",
    "    new_color_start = np.array([colormap(0)]*n_start).reshape(-1, new_colors_range.shape[1])\n",
    "    n_finish = round(nb_colors/(1-percent_finish)) - nb_colors if percent_finish != 0 else 0\n",
    "    new_color_finish = np.array([colormap(0)]*n_finish).reshape(-1, new_colors_range.shape[1])\n",
    "\n",
    "    new_colors_range = np.concatenate((new_color_start,new_colors_range,new_color_finish), axis=0)\n",
    "    new_colormap = colors.ListedColormap(new_colors_range)\n",
    "    return new_colormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2de3bd8-6b88-45b9-84e2-eae03fff2629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sub_models(sub, scale, conv, models_path, no_init=False): \n",
    "    models = {}\n",
    "    #scale_path = os.path.join(models_path, sub, scale)\n",
    "    for model in os.listdir(models_path):\n",
    "        if '.pt' in model and conv in model and sub in model and scale in model:\n",
    "            model_path = os.path.join(models_path, model)\n",
    "            modeldict = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "            model_net = encod.SoundNetEncoding_conv(out_size=modeldict['out_size'],output_layer=modeldict['output_layer'],\n",
    "                                                    kernel_size=modeldict['kernel_size'], no_init=no_init)\n",
    "            if not no_init:\n",
    "                model_net.load_state_dict(modeldict['checkpoint'])\n",
    "            models[model] = model_net\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "400f4592-4439-409d-8428-90022ceb5434",
   "metadata": {},
   "outputs": [],
   "source": [
    "class movie10_dataset(Dataset):\n",
    "    def __init__(self, data, temporal_window, tr, sr):\n",
    "        self.temporal_window = temporal_window\n",
    "        self.tr = tr\n",
    "        self.sr = sr\n",
    "\n",
    "        data_by_temporal_window = []\n",
    "        for (run_wav, run_bold) in data:  \n",
    "            run_data = self.__create_temporal_segments__(run_wav, run_bold)\n",
    "            data_by_temporal_window.extend(run_data)\n",
    "\n",
    "        self.x = [wav for (wav, bold) in data_by_temporal_window]\n",
    "        self.y = [bold for (wav, bold) in data_by_temporal_window]\n",
    "        \n",
    "    def __create_temporal_segments__(self, wav, bold):\n",
    "        chunk_length = round(self.sr*self.tr)*self.temporal_window\n",
    "        wav_length = len(wav)\n",
    "        \n",
    "        wav_starts = range(0, len(wav), chunk_length)\n",
    "        bold_starts = range(0, len(bold), self.temporal_window)\n",
    "        wavbold_by_temporalwindow = []\n",
    "        for wav_start, bold_start in zip(wav_starts, bold_starts):\n",
    "            \n",
    "            if wav_start+chunk_length < len(wav):            \n",
    "                wav_chunk = wav[wav_start:wav_start+chunk_length]\n",
    "                bold_chunk = bold[bold_start:bold_start+self.temporal_window,:]\n",
    "            else:\n",
    "                wav_tr = round((len(wav) - wav_start)/(self.tr*self.sr))\n",
    "                bold_tr = len(bold) - bold_start\n",
    "                min_tr = wav_tr if wav_tr <= bold_tr else bold_tr\n",
    "    \n",
    "                wav_stop = int(wav_start+min_tr*self.sr*self.tr)\n",
    "                wav_chunk = wav[wav_start:wav_stop]\n",
    "                bold_chunk = bold[bold_start:bold_start+min_tr,:]  \n",
    "                \n",
    "            wavbold_by_temporalwindow.append((wav_chunk, bold_chunk))\n",
    "    \n",
    "        return wavbold_by_temporalwindow\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d24a100-3cce-4a1f-b800-d84f43c32b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, net, epoch, mseloss, return_nodes, gpu=True):\n",
    "    net.eval()\n",
    "    out_p = {layer_name:[] for layer_name in return_nodes.values()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for (x,y) in dataloader:\n",
    "            #print(x.shape, y.shape)\n",
    "            # load data\n",
    "            x = torch.Tensor(x).view(1,1,-1,1)\n",
    "            # Forward pass\n",
    "            y_p = net(x, epoch)\n",
    "            \n",
    "            for key, p in y_p.items():\n",
    "                #print(val.shape)\n",
    "                p = p.permute(2,1,0,3).squeeze()\n",
    "                #print(p.shape, y.shape)\n",
    "                out_p[key].append((p.numpy(), y.numpy()))\n",
    "    return out_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "838bcf4d-83ff-4098-afc5-3583dda81c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of encoding matrice from last encoding layer : 1024 X 210\n",
      "nb of tr in temporal window:  70\n",
      "    Unnamed: 0 category       task  repetition\n",
      "0            0     wolf     wolf09         NaN\n",
      "1            1   bourne   bourne09         NaN\n",
      "2            2  figures  figures03         1.0\n",
      "3            3     wolf     wolf04         NaN\n",
      "4            4     wolf     wolf03         NaN\n",
      "..         ...      ...        ...         ...\n",
      "56          56     life     life04         1.0\n",
      "57          57     wolf     wolf05         NaN\n",
      "58          58   bourne   bourne04         NaN\n",
      "59          59  figures  figures08         1.0\n",
      "60          60     wolf     wolf11         NaN\n",
      "\n",
      "[61 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "#necessary args\n",
    "dataset = 'movie10'\n",
    "sub = 'sub-06'\n",
    "no_init = False\n",
    "conv = 'conv4' #'opt110_wb'#, 'sub-02', 'sub-03', 'sub-04', 'sub-05'\n",
    "scale = 'MIST_ROI'#, 'auditory_Voxels' \n",
    "shape = 210\n",
    "\n",
    "models_path = '/home/maellef/Results/best_models/converted' \n",
    "training_data_path = '/home/maellef/git/MuteMusic_analysis/data/training_data'\n",
    "\n",
    "#load and extract model from dict\n",
    "models = load_sub_models(sub, scale, conv, models_path, no_init=no_init)\n",
    "\n",
    "for name, model in models.items():\n",
    "    i = name.find('conv_') + len('conv_')\n",
    "    temporal_size = int(name[i:i+3]) \n",
    "    model = model\n",
    "\n",
    "print('nb of tr in temporal window: ', temporal_size)\n",
    "#load data + metadata\n",
    "metadata_path = os.path.join(training_data_path, f'{dataset}_{sub}_metadata.tsv')\n",
    "pairbold_path = os.path.join(training_data_path, f'{dataset}_{sub}_pairWavBold')\n",
    "\n",
    "data_df = pandas.read_csv(metadata_path, sep='\\t')\n",
    "with open(pairbold_path, 'rb') as f: \n",
    "    wavbold = pickle.load(f)\n",
    "\n",
    "print(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f50a1717-94d9-4f8c-a043-1cfd3c615f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  3  4 12 18 21 23 25 29 44 47 48 51 52 53 57 60]\n",
      "61\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "#select data depending on metadata infos\n",
    "category = 'wolf'\n",
    "repetition = 'all'\n",
    "\n",
    "cat = data_df['category'].unique() if category == 'all' else [category]\n",
    "rep = data_df['repetition'].unique() if repetition == 'all' else [repetition]\n",
    "i_selection = data_df.loc[(data_df['category'].isin(cat))\n",
    "                            &(data_df['repetition'].isin(rep))].index.values\n",
    "print(i_selection)\n",
    "print(len(wavbold))\n",
    "selected_wavbold = [(wav, bold) for i, (wav, bold) in enumerate(wavbold) if i in i_selection]\n",
    "print(len(selected_wavbold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58ecf3ef-ec1f-4c45-8b14-4846f6f17f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  3  4 12 18 21 23 25 29 44 47 48 51 52 53 57 60]\n"
     ]
    }
   ],
   "source": [
    "def select_df_index(df, **selectors):\n",
    "    '''return the rows indexes of a dataframe based on selectors\n",
    "    selector argument : column name = value to select\n",
    "    value can be a list of values, or a single value\n",
    "    if value = 'all', all rows for this column will be selected'''\n",
    "    \n",
    "    conditions = True\n",
    "    for column_name, val in selectors.items():\n",
    "        val = val if isinstance(val, list) else [val]\n",
    "        selected_items = df[column_name].unique() if val[0]=='all' else val\n",
    "        condition = df[column_name].isin(selected_items)\n",
    "        conditions &= (condition)\n",
    "    i_selection = df.loc[conditions].index.values\n",
    "    return i_selection\n",
    "\n",
    "i = select_df_index(data_df, category='wolf', repetition='all')\n",
    "\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2c5636-4eb7-450b-8a20-b4bb77b4d556",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract X and Y data for prediction + check for empty data (WIP: move to previous later)\n",
    "empty_pair = []\n",
    "for i, (wav, bold) in enumerate(selected_wavbold):\n",
    "    if wav.shape[0] == 0 and bold.shape[0] == 0:\n",
    "        empty_pair.append(i)\n",
    "\n",
    "correct_wavbold = [(wav, bold) for (wav, bold) in selected_wavbold if wav.shape[0] != 0]\n",
    "print(empty_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41346255-71bb-4687-a1e2-b82759368c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define all possible output (WIP) + create model with extractable embeddings\n",
    "train_nodes, eval_nodes = feature_extraction.get_graph_node_names(model)\n",
    "#return_nodes = {layer:layer[len('soundnet.'):-2] for layer in train_nodes if layer[-1] == '2'}\n",
    "print(eval_nodes)\n",
    "\n",
    "return_nodes = {'soundnet.conv7.2':'conv7', 'encoding_fmri':'encoding_conv'}\n",
    "model_feat = feature_extraction.create_feature_extractor(model, return_nodes=return_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d40deb-2d87-4a8d-876f-aab8f60b75b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataset for dataloader\n",
    "model_dataset = movie10_dataset(correct_wavbold, temporal_window=temporal_size, tr=1.49, sr=22050)\n",
    "testloader = DataLoader(model_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65657dcd-f40b-4c65-90e5-30e14bec5a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract embedding from selected model\n",
    "out_p = test(testloader, net=model_feat, epoch=1, \n",
    "     mseloss=nn.MSELoss(reduction='sum'), \n",
    "     return_nodes=return_nodes, gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7f0f4a-40a3-44b4-8fed-68e580584677",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out_p['encoding_conv'][0][0].shape, out_p['encoding_conv'][0][1].shape)\n",
    "\n",
    "predicted_y = [pred_y[:y.shape[1],:] for (pred_y, y) in out_p['encoding_conv']]\n",
    "predicted_y = np.vstack(predicted_y)\n",
    "\n",
    "real_y = [y.squeeze() for (pred_y, y) in out_p['encoding_conv']]\n",
    "real_y = np.vstack(real_y)\n",
    "\n",
    "print(predicted_y.shape, real_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1ee698-6dc1-4d38-85be-c062e1c9b354",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = r2_score(real_y, predicted_y, multioutput='raw_values')\n",
    "r2 = np.where(r2<0, 0, r2)\n",
    "print(max(r2))\n",
    "colormap = extend_colormap(original_colormap='turbo',\n",
    "                          percent_start = 0.1, percent_finish=0)\n",
    "surface_fig(r2, vmax=0.25, threshold=0.00005, cmap='turbo', symmetric_cbar=False)\n",
    "\n",
    "savepath = f'./figures/{sub}_generalisation_{dataset}_{category}_{repetition}.png'\n",
    "plt.savefig(savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc0c886-9a44-49d3-b269-ae352ddacee2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MISTinfo_path = '/home/maellef/DataBase/fMRI_parcellations/MIST_parcellation/Parcel_Information/MIST_ROI.csv'\n",
    "MIST_df = pandas.read_csv(MISTinfo_path, sep=';')\n",
    "\n",
    "min = []\n",
    "imin = []\n",
    "for i in range(len(r2)):\n",
    "    if r2[i] < -2:\n",
    "        min.append(r2[i])\n",
    "        imin.append(i)\n",
    "print(imin, min)\n",
    "print(MIST_df.iloc[imin]['name'])\n",
    "print(MIST_df.iloc[48]['name'])\n",
    "print(r2[48])\n",
    "print(MIST_df.iloc[49]['name'])\n",
    "print(r2[49])\n",
    "print(MIST_df.iloc[208]['name'])\n",
    "print(r2[208])\n",
    "print(MIST_df.iloc[209]['name'])\n",
    "print(r2[209])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
